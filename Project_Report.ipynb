{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capstone Project - Car Accident Severity (Week 1)\n",
    "### Applied Data Science Capstone by IBM/Coursera"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction: Business Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The aim of this project is to design a model to predict the probability of a severe car accident occurring on a road given the right data. The stakeholders interested in this project are the drivers and pedestrians who pass through the affected roads at a given time.  \n",
    "\n",
    "We would use some data science tools and methodologies coupled with some machine learning algorithms to design, train, and evaluate our model. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset has 194673 rows and 38 columns including Location, Severity Code, Weather, Road Condition etc. and this is made available in a CSV format.\n",
    "Our predictor or target variable will be 'SEVERITYCODE' because this is a measure of the severity of an accident from 0 to 5 within the dataset. We will use 'WEATHER', 'ROADCOND' and 'LIGHTCOND'to determine the severity of an accident."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation  \n",
    "The dataset in the original form has some obscurities and as such, it cannot be used for data analysis. So, we employ some data wrangling techniques such as, feature extraction, oversampling standardization, normalization, and data encoding. \n",
    "Since we are to predict the severity of car accidents, our focus is drawn to the attributes which directly affect the severity of an accident. For our dataset, they are - severity, weather conditions, road conditions, and light conditions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methodology"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During the exploratory data analysis some columns were found to have lots of missing data and were dropped since these columns were not required for our data analysis. Relevant categorical variables were normalized by replacing with average value of entire value, drop unwanted rows/columns, replace missing values with most frequent value in the column and down sampling the majority class with sklearn's resample tool. No statistical testing was performed because the data revolved around categorical variables, not numerical ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling  \n",
    "In this phase, various algorithms and methods are selected and applied to build the model including supervised machine learning techniques. I have selected, K nearest neighbor, Decision tree, logistic regression and SVM. The data is split, trained with the above listed algorithms and tested. After evaluating the results, the best classification algorithm can be determined."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results and Evaluation  \n",
    "Before proceeding to the deployment stage, the model is evaluated thoroughly to ensure that the business and/or the applications' objectives are achieved. The metrics used for the model evaluation are accuracy, F1-score and Jaccard similarity score.\n",
    "\n",
    "From the results , decision tree has proven to the best algorithm for predicting severity of car accidents with an accuracy score of 74%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After extracting the dataset into a pandas data frame. Majority of the data in the dataset were of type ‘object’ which could not have been used for data analysis. Data encoding was used to create labels of datatype ‘int’. Majority of the data in the dataset were imbalanced and had to be normalized. Methods used to normalize the data were, replacing with average value of entire value, drop unwanted rows/columns, replace missing values with most frequent value in the column and down sampling the majority class with sklearn's resample tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
